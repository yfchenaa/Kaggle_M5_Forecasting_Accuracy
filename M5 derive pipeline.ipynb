{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from derive_functions.derive_date_var import derive_calender_feats\n",
    "from derive_functions.data_prepare_func import create_dt,reduce_mem_usage\n",
    "from derive_functions.derive_lag_mean_feats import create_lag_feats,create_lag_mean_feats\n",
    "from derive_functions.derive_mean_feats_cat import get_df_cat\n",
    "from derive_functions.derive_mean_feats_dept import get_df_dept\n",
    "from derive_functions.derive_mean_feats_id import get_df_id\n",
    "from derive_functions.derive_mean_feats_item import get_df_item\n",
    "from derive_functions.derive_mean_feats_state import get_df_state\n",
    "from derive_functions.derive_mean_feats_store import get_df_store\n",
    "from derive_functions.derive_deviation_feats import get_deviation_feats\n",
    "import gc\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. 日期数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_calendar = pd.read_csv('data/raw_data/calendar.csv')\n",
    "data_calender_extended = pd.read_csv('data/calendar_extended.csv')\n",
    "data_calendar_full = derive_calender_feats(df_calender, df_calender_extended)\n",
    "data_calendar_full.to_csv('data/raw_data/calendar_full.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 读取数据切分训练测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sales_train_evaluation = pd.read_csv('data/raw_data/sales_train_evaluation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES={\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n",
    "         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n",
    "        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 28 \n",
    "max_lags = 120\n",
    "# 訓練的最後一天:d_1941\n",
    "tr_last = 1941\n",
    "fday = datetime.datetime(2016,5,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_dt(is_train=True, first_day=1090, tr_last=1941, h=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2588.49 Mb (68.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 719.04 Mb (69.6% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_test = create_dt(is_train=False, first_day= 1200, tr_last=1941, h=28)\n",
    "df_test = reduce_mem_usage(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_list = ['is_religious_holiday',\n",
    "       'is_national_holiday', 'is_cultural_holiday', 'is_sporting_holiday',\n",
    "       'num_holidays', 'ReligiousHolidayNextMonth',\n",
    "       'ReligiousHolidayLastMonth', 'ReligiousHolidayThisMonth',\n",
    "       'ReligiousHolidayNextWeek', 'ReligiousHolidayLastWeek',\n",
    "       'ReligiousHolidayThisWeek', 'NationalHolidayNextMonth',\n",
    "       'NationalHolidayLastMonth', 'NationalHolidayThisMonth',\n",
    "       'NationalHolidayNextWeek', 'NationalHolidayLastWeek',\n",
    "       'NationalHolidayThisWeek', 'CulturalHolidayNextMonth',\n",
    "       'CulturalHolidayLastMonth', 'CulturalHolidayThisMonth',\n",
    "       'CulturalHolidayNextWeek', 'CulturalHolidayLastWeek',\n",
    "       'CulturalHolidayThisWeek', 'SportingHolidayNextMonth',\n",
    "       'SportingHolidayLastMonth', 'SportingHolidayThisMonth',\n",
    "       'SportingHolidayNextWeek', 'SportingHolidayLastWeek',\n",
    "       'SportingHolidayThisWeek', 'AllHolidayNextMonth', 'AllHolidayLastMonth',\n",
    "       'AllHolidayThisMonth', 'AllHolidayNextWeek', 'AllHolidayLastWeek',\n",
    "       'AllHolidayThisWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in var_list:\n",
    "    df_train[var] = df_train[var].astype(\"int8\")\n",
    "for var in var_list:\n",
    "    df_test[var] = df_test[var].astype(\"int8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('data/processed_data/df_train_v2.pkl')\n",
    "df_test.to_pickle('data/processed_data/df_test_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 变量衍生"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('data/processed_data/df_train_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25130703, 58)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对测试集进行衍生\n",
    "# data_test = pd.read_pickle('data/processed_data/df_test_v2.pkl')\n",
    "# data_train = data_test.copy(deep=True)\n",
    "# del data_test\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['day'] = [date.day for date in data_train['date']]\n",
    "data_train['week_of_yr'] = [date.week for date in data_train['date']]\n",
    "data_train['is_weekend'] = np.where(data_train['wday'].isin([1,2]),1,0)\n",
    "data_train['quarter'] = np.where(data_train['month'].isin([1,2,3]),1,\n",
    "                                np.where(data_train['month'].isin([4,5,6]),2,\n",
    "                                         np.where(data_train['month'].isin([7,8,9]),3,4)))\n",
    "data_train['is_quarter_end'] = np.where((((data_train['month']==3) & (data_train['day'].isin([25,26,27,28,29,30,31])))|\n",
    "                                        ((data_train['month']==6) & (data_train['day'].isin([25,26,27,28,29,30,24])))|\n",
    "                                        ((data_train['month']==9) & (data_train['day'].isin([25,26,27,28,29,30,24])))|\n",
    "                                        ((data_train['month']==12) & (data_train['day'].isin([25,26,27,28,29,30,31])))),1,0)\n",
    "data_train['snap'] = np.where((((data_train['state_id'] == 'CA')& (data_train['snap_CA']==1))|\n",
    "                              ((data_train['state_id'] == 'TX')& (data_train['snap_TX']==1))|\n",
    "                              ((data_train['state_id'] == 'WI')& (data_train['snap_WI']==1))),1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2015-11-04 00:00:00')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.date.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 mean feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cat, df_cat_id_snap_mean, df_cat_id_weekend_mean, df_cat_id_qtr_end_mean = get_df_cat(data_train)\n",
    "df_cat.to_pickle('data/processed_data/df_cat.pkl')\n",
    "df_cat_id_snap_mean.to_pickle('data/processed_data/df_cat_id_snap_mean.pkl')\n",
    "df_cat_id_weekend_mean.to_pickle('data/processed_data/df_cat_id_weekend_mean.pkl')\n",
    "df_cat_id_qtr_end_mean.to_pickle('data/processed_data/df_cat_id_qtr_end_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_state, df_state_id_snap_mean, df_state_id_weekend_mean, df_state_id_qtr_end_mean = get_df_state(data_train)\n",
    "df_state.to_pickle('data/processed_data/df_state.pkl')\n",
    "df_state_id_snap_mean.to_pickle('data/processed_data/df_state_id_snap_mean.pkl')\n",
    "df_state_id_weekend_mean.to_pickle('data/processed_data/df_state_id_weekend_mean.pkl')\n",
    "df_state_id_qtr_end_mean.to_pickle('data/processed_data/df_state_id_qtr_end_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store, df_store_id_snap_mean, df_store_id_weekend_mean, df_store_id_qtr_end_mean = get_df_store(data_train)\n",
    "df_store.to_pickle('data/processed_data/df_store.pkl')\n",
    "df_store_id_snap_mean.to_pickle('data/processed_data/df_store_id_snap_mean.pkl')\n",
    "df_store_id_weekend_mean.to_pickle('data/processed_data/df_store_id_weekend_mean.pkl')\n",
    "df_store_id_qtr_end_mean.to_pickle('data/processed_data/df_store_id_qtr_end_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dept, df_dept_id_snap_mean, df_dept_id_weekend_mean, df_dept_id_qtr_end_mean = get_df_dept(data_train)\n",
    "df_dept.to_pickle('data/processed_data/df_dept.pkl')\n",
    "df_dept_id_snap_mean.to_pickle('data/processed_data/df_dept_id_snap_mean.pkl')\n",
    "df_dept_id_weekend_mean.to_pickle('data/processed_data/df_dept_id_weekend_mean.pkl')\n",
    "df_dept_id_qtr_end_mean.to_pickle('data/processed_data/df_dept_id_qtr_end_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_id, df_id_snap_mean, df_id_weekend_mean, df_id_qtr_end_mean = get_df_id(data_train)\n",
    "df_id.to_pickle('data/processed_data/df_id.pkl')\n",
    "df_id_snap_mean.to_pickle('data/processed_data/df_id_snap_mean.pkl')\n",
    "df_id_weekend_mean.to_pickle('data/processed_data/df_id_weekend_mean.pkl')\n",
    "df_id_qtr_end_mean.to_pickle('data/processed_data/df_id_qtr_end_mean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item, df_item_snap_mean, df_item_weekend_mean, df_item_qtr_end_mean,df_item_price_mode = get_df_item(data_train)\n",
    "df_item.to_pickle('data/processed_data/df_item.pkl')\n",
    "df_item_snap_mean.to_pickle('data/processed_data/df_item_snap_mean.pkl')\n",
    "df_item_weekend_mean.to_pickle('data/processed_data/df_item_weekend_mean.pkl')\n",
    "df_item_qtr_end_mean.to_pickle('data/processed_data/df_item_qtr_end_mean.pkl')\n",
    "df_item_price_mode.to_pickle('data/processed_data/df_item_price_mode.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 lag feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = create_lag_feats(data_train,recursive=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 lag mean feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_mean_feats...\n",
      "generating sales_per_id_lag_7_r_7_mean\n",
      "generating sales_per_id_lag_7_r_28_mean\n",
      "generating sales_per_id_lag_7_r_90_mean\n",
      "generating sales_per_id_lag_28_r_7_mean\n",
      "generating sales_per_id_lag_28_r_28_mean\n",
      "generating sales_per_id_lag_28_r_90_mean\n",
      "generating revenue_per_id_lag_7_r_7_mean\n",
      "generating revenue_per_id_lag_7_r_28_mean\n",
      "generating revenue_per_id_lag_7_r_90_mean\n",
      "generating revenue_per_id_lag_28_r_7_mean\n",
      "generating revenue_per_id_lag_28_r_28_mean\n",
      "generating revenue_per_id_lag_28_r_90_mean\n",
      "get_median_feats...\n",
      "generating sell_price_per_id_lag_7_r_7_median\n",
      "generating sell_price_per_id_lag_7_r_28_median\n",
      "generating sell_price_per_id_lag_7_r_90_median\n",
      "generating sell_price_per_id_lag_28_r_7_median\n",
      "generating sell_price_per_id_lag_28_r_28_median\n",
      "generating sell_price_per_id_lag_28_r_90_median\n",
      "get deviation feats...\n"
     ]
    }
   ],
   "source": [
    "data_train = create_lag_mean_feats(data_train,recursive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1025.26 Mb (14.4% reduction)\n"
     ]
    }
   ],
   "source": [
    "data_train = reduce_mem_usage(data_train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_train.to_pickle('data/processed_data/df_train_before_merge_v2.pkl')\n",
    "data_train.to_pickle('data/processed_data/df_test_before_merge_v2.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 merge 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_pickle('data/processed_data/df_train_before_merge_v2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 34.54 Mb (27.3% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (35.7% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (46.5% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (46.5% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (46.5% reduction)\n",
      "Mem. usage decreased to  3.84 Mb (46.8% reduction)\n",
      "Mem. usage decreased to  0.38 Mb (46.3% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (21.3% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (26.0% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (40.1% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (36.5% reduction)\n",
      "Mem. usage decreased to  1.22 Mb (34.4% reduction)\n",
      "Mem. usage decreased to  0.06 Mb (50.0% reduction)\n",
      "Mem. usage decreased to  0.06 Mb (50.0% reduction)\n",
      "Mem. usage decreased to  1.22 Mb (34.4% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (36.5% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (40.1% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (26.0% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (21.3% reduction)\n",
      "Mem. usage decreased to  0.06 Mb (50.0% reduction)\n",
      "Mem. usage decreased to  1.22 Mb (34.4% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (36.5% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (40.1% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (26.0% reduction)\n",
      "Mem. usage decreased to  0.00 Mb (21.3% reduction)\n"
     ]
    }
   ],
   "source": [
    "# data_train = pd.read_pickle('data/processed_data/df_train_before_merge.pkl')\n",
    "\n",
    "df_cat = pd.read_pickle('data/processed_data/mean_feats/df_cat.pkl')\n",
    "df_cat_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_cat_id_snap_mean.pkl')\n",
    "df_cat_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_cat_id_weekend_mean.pkl')\n",
    "df_cat_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_cat_id_qtr_end_mean.pkl')\n",
    "\n",
    "df_state = pd.read_pickle('data/processed_data/mean_feats/df_state.pkl')\n",
    "df_state_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_state_id_snap_mean.pkl')\n",
    "df_state_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_state_id_weekend_mean.pkl')\n",
    "df_state_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_state_id_qtr_end_mean.pkl')\n",
    "\n",
    "\n",
    "df_store = pd.read_pickle('data/processed_data/mean_feats/df_store.pkl')\n",
    "df_store_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_store_id_snap_mean.pkl')\n",
    "df_store_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_store_id_weekend_mean.pkl')\n",
    "df_store_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_store_id_qtr_end_mean.pkl')\n",
    "\n",
    "df_dept = pd.read_pickle('data/processed_data/mean_feats/df_dept.pkl')\n",
    "df_dept_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_dept_id_snap_mean.pkl')\n",
    "df_dept_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_dept_id_weekend_mean.pkl')\n",
    "df_dept_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_dept_id_qtr_end_mean.pkl')\n",
    "\n",
    "df_id = pd.read_pickle('data/processed_data/mean_feats/df_id.pkl')\n",
    "df_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_id_snap_mean.pkl')\n",
    "df_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_id_weekend_mean.pkl')\n",
    "df_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_id_qtr_end_mean.pkl')\n",
    "\n",
    "df_item = pd.read_pickle('data/processed_data/mean_feats/df_item.pkl')\n",
    "df_item_id_snap_mean = pd.read_pickle('data/processed_data/mean_feats/df_item_snap_mean.pkl')\n",
    "df_item_id_weekend_mean = pd.read_pickle('data/processed_data/mean_feats/df_item_weekend_mean.pkl')\n",
    "df_item_id_qtr_end_mean = pd.read_pickle('data/processed_data/mean_feats/df_item_qtr_end_mean.pkl')\n",
    "df_item_id_price_mode = pd.read_pickle('data/processed_data/mean_feats/df_item_price_mode.pkl')\n",
    "df_item_id_price_mode = reduce_mem_usage(df_item_id_price_mode, verbose=True)\n",
    "\n",
    "\n",
    "df_cat = reduce_mem_usage(df_cat, verbose=True)\n",
    "df_state = reduce_mem_usage(df_state, verbose=True)\n",
    "df_store = reduce_mem_usage(df_store, verbose=True)\n",
    "df_dept = reduce_mem_usage(df_dept, verbose=True)\n",
    "df_id = reduce_mem_usage(df_id, verbose=True)\n",
    "df_item = reduce_mem_usage(df_item, verbose=True)\n",
    "\n",
    "\n",
    "df_cat_id_snap_mean = reduce_mem_usage(df_cat_id_snap_mean, verbose=True)\n",
    "df_state_id_snap_mean = reduce_mem_usage(df_state_id_snap_mean, verbose=True)\n",
    "df_store_id_snap_mean = reduce_mem_usage(df_store_id_snap_mean, verbose=True)\n",
    "df_dept_id_snap_mean = reduce_mem_usage(df_dept_id_snap_mean, verbose=True)\n",
    "df_id_snap_mean = reduce_mem_usage(df_id_snap_mean, verbose=True)\n",
    "df_item_id_snap_mean = reduce_mem_usage(df_item_id_snap_mean, verbose=True)\n",
    "\n",
    "df_item_id_weekend_mean = reduce_mem_usage(df_item_id_weekend_mean, verbose=True)\n",
    "df_id_weekend_mean = reduce_mem_usage(df_id_weekend_mean, verbose=True)\n",
    "df_dept_id_weekend_mean = reduce_mem_usage(df_dept_id_weekend_mean, verbose=True)\n",
    "df_store_id_weekend_mean = reduce_mem_usage(df_store_id_weekend_mean, verbose=True)\n",
    "df_state_id_weekend_mean = reduce_mem_usage(df_state_id_weekend_mean, verbose=True)\n",
    "df_cat_id_weekend_mean = reduce_mem_usage(df_cat_id_weekend_mean, verbose=True)\n",
    "\n",
    "df_item_id_qtr_end_mean = reduce_mem_usage(df_item_id_qtr_end_mean, verbose=True)\n",
    "df_id_qtr_end_mean = reduce_mem_usage(df_id_qtr_end_mean, verbose=True)\n",
    "df_dept_id_qtr_end_mean = reduce_mem_usage(df_dept_id_qtr_end_mean, verbose=True)\n",
    "df_store_id_qtr_end_mean = reduce_mem_usage(df_store_id_qtr_end_mean, verbose=True)\n",
    "df_state_id_qtr_end_mean = reduce_mem_usage(df_state_id_qtr_end_mean, verbose=True)\n",
    "df_cat_id_qtr_end_mean = reduce_mem_usage(df_cat_id_qtr_end_mean, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1 删除多余的列，不然内存报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for var in df_cat.columns:\n",
    "    if ('avg_revenue_per_cat_' in var) or ('qtr' in var):\n",
    "        del df_cat[var]\n",
    "        gc.collect()\n",
    "           \n",
    "for var in df_dept.columns:\n",
    "    if ('avg_revenue_per_dept_' in var) or ('qtr' in var):\n",
    "        del df_dept[var]\n",
    "        gc.collect()\n",
    "        \n",
    "for var in df_store.columns:\n",
    "    if ('avg_revenue_per_store_' in var) or ('qtr' in var):\n",
    "        del df_store[var]\n",
    "        gc.collect()\n",
    "        \n",
    "        \n",
    "for var in df_state.columns:\n",
    "    if ('avg_revenue_per_state_' in var) or ('qtr' in var):\n",
    "        del df_state[var]\n",
    "        gc.collect()\n",
    "        \n",
    "for var in df_item.columns:\n",
    "    if ('avg_revenue_per_item_' in var) or ('qtr' in var):\n",
    "        del df_item[var]\n",
    "        gc.collect()\n",
    "        \n",
    "for var in df_id.columns:\n",
    "    if ('avg_revenue_per_id_' in var) or ('qtr' in var):\n",
    "        del df_id[var]\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 merge p1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(data_train, df_store, on='store_id', how='left')\n",
    "del data_train\n",
    "gc.collect()\n",
    "df_train = pd.merge(df_train, df_dept, on='dept_id', how='left')\n",
    "df_train = pd.merge(df_train, df_id, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3 merge p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.merge(df_train, df_store_id_snap_mean, on=['snap','store_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_store_id_weekend_mean, on=['is_weekend','store_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_store_id_qtr_end_mean, on=['is_quarter_end','store_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_dept_id_snap_mean, on=['snap','dept_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_dept_id_weekend_mean, on=['is_weekend','dept_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_dept_id_qtr_end_mean, on=['is_quarter_end','dept_id'], how='left')\n",
    "df_train = pd.merge(df_train, df_id_snap_mean, on=['snap','id'], how='left')\n",
    "df_train = pd.merge(df_train, df_id_weekend_mean, on=['is_weekend','id'], how='left')\n",
    "df_train = pd.merge(df_train, df_id_qtr_end_mean, on=['is_quarter_end','id'], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. deviation feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2582.96 Mb (0.0% reduction)\n"
     ]
    }
   ],
   "source": [
    "df_train = get_deviation_feats(df_train)\n",
    "df_train = reduce_mem_usage(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_pickle('data/processed_data/df_train_var_all_0625.pkl')\n",
    "df_train.to_pickle('data/processed_data/df_test_var_all_0625.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 合并每个item的价格众数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/processed_data/df_train_var_all_0625.pkl')\n",
    "df_train = df_train[df_train.date>='2014-05-20']\n",
    "gc.collect()\n",
    "useless_cols = ['d','wm_yr_wk','weekday']\n",
    "for var in useless_cols:\n",
    "    del df_train[var]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_item_id_price_mode = pd.read_pickle('data/processed_data/mean_feats/df_item_price_mode_train.pkl')\n",
    "df_item_id_price_mode = reduce_mem_usage(df_item_id_price_mode, verbose=True)\n",
    "df_train = pd.merge(df_train,df_item_id_price_mode[['item_id', 'date', 'item_sell_price_mode']],\n",
    "                    on=['item_id','date'],how='left')\n",
    "gc.collect()\n",
    "\n",
    "df_train['item_sell_price_mode_dev'] = (df_train['sell_price']-df_train['item_sell_price_mode'])/df_train['item_sell_price_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出训练数据集\n",
    "df_train.to_pickle('data/processed_data/df_train_var_all_0626.pkl')\n",
    "df_train[df_train.state_id=='TX'].to_pickle('data/processed_data/df_train_var_all_0626_TX_subset.pkl')\n",
    "df_train[df_train.state_id=='WI'].to_pickle('data/processed_data/df_train_var_all_0626_WI_subset.pkl')\n",
    "df_train[(df_train.state_id=='CA') & (df_train.date>='2014-09-06')].to_pickle('data/processed_data/df_train_var_all_0626_CA_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 输出测试数据集\n",
    "df_test = pd.read_pickle('data/processed_data/df_test_var_all_0625.pkl')\n",
    "df_item_price_mode = df_test[['item_id', 'date', 'sell_price']].groupby(['item_id', 'date'], as_index=False).agg(\n",
    "    lambda x: x.value_counts().index[0])\n",
    "df_item_price_mode.rename(columns={'sell_price': 'item_sell_price_mode'}, inplace=True)\n",
    "df_item_price_mode.to_pickle('data/processed_data/mean_feats/df_item_price_mode_test.pkl')\n",
    "\n",
    "df_test = pd.merge(df_test,df_item_price_mode[['item_id', 'date', 'item_sell_price_mode']],\n",
    "                    on=['item_id','date'],how='left')\n",
    "df_test['item_sell_price_mode_dev'] = (df_test['sell_price']-df_test['item_sell_price_mode'])/df_test['item_sell_price_mode']\n",
    "df_test.to_pickle('data/processed_data/df_test_var_all_0626.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
